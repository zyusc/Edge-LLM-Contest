{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neelesh/anaconda3/envs/nemo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "import warnings\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf\n",
    "import cudf\n",
    "import gzip\n",
    "import json\n",
    "import dask.bag as db\n",
    "import glob\n",
    "from dask.distributed import wait\n",
    "import numpy as np\n",
    "\n",
    "from nemo_curator import get_client\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import (\n",
    "    get_num_workers,\n",
    "    read_data,\n",
    "    write_to_disk,\n",
    ")\n",
    "from nemo_curator.utils.file_utils import (\n",
    "    expand_outdir_and_mkdir, \n",
    "    get_all_files_paths_under, \n",
    "    separate_by_metadata,\n",
    "    get_batched_files,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "base_dir = \"/home/neelesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n"
     ]
    }
   ],
   "source": [
    "sched_addr = os.environ.get('SCHEDULER_ADDRESS')\n",
    "gpu_client = get_client(cluster_type = 'gpu', set_torch_to_use_rmm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import MinHash\n",
    "\n",
    "input_data_dir = os.path.join(base_dir,\"clean_c4_en_cleaned\")\n",
    "seed = 42\n",
    "minhash_length = 260\n",
    "char_ngram = 5\n",
    "log_dir = expand_outdir_and_mkdir(os.path.join(base_dir, \"logs\"))\n",
    "id_field = 'id'\n",
    "text_field = 'text'\n",
    "minshah_output_dir = expand_outdir_and_mkdir(os.path.join(base_dir,\"00_c4_minhash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1792 files\n"
     ]
    }
   ],
   "source": [
    "files = get_all_files_paths_under(root=input_data_dir, recurse_subdirectories=False)\n",
    "files = [f for f in files if f.endswith(\".jsonl\")]\n",
    "df = read_data(\n",
    "    files,\n",
    "    file_type=\"jsonl\",\n",
    "    backend=\"cudf\",\n",
    "    files_per_partition=1,\n",
    "    add_filename=False,\n",
    ")[[id_field, text_field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing minhashes took:321.2562484741211\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Run MinHash() on input data\n",
    "minhasher = MinHash(\n",
    "    seed=seed,\n",
    "    num_hashes=minhash_length,\n",
    "    char_ngrams=char_ngram,\n",
    "    use_64bit_hash=False,\n",
    "    logger=log_dir,\n",
    "    id_field=id_field,\n",
    "    text_field=text_field,\n",
    "    cache_dir=minshah_output_dir\n",
    ")\n",
    "\n",
    "result = minhasher(DocumentDataset(df)).df\n",
    "\n",
    "print(f\"Computing minhashes took:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c4-train-0000000000</td>\n",
       "      <td>[30032382, 157261, 5008033, 4311555, 19755091,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4-train-0000000001</td>\n",
       "      <td>[511522, 1015487, 2320335, 651428, 1906819, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c4-train-0000000002</td>\n",
       "      <td>[15994705, 6370213, 15559465, 9740304, 6210120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4-train-0000000003</td>\n",
       "      <td>[1449615, 1872293, 3654170, 452331, 780352, 39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4-train-0000000004</td>\n",
       "      <td>[6685476, 2415781, 1112347, 742646, 6898911, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                 _minhash_signature\n",
       "0  c4-train-0000000000  [30032382, 157261, 5008033, 4311555, 19755091,...\n",
       "1  c4-train-0000000001  [511522, 1015487, 2320335, 651428, 1906819, 14...\n",
       "2  c4-train-0000000002  [15994705, 6370213, 15559465, 9740304, 6210120...\n",
       "3  c4-train-0000000003  [1449615, 1872293, 3654170, 452331, 780352, 39...\n",
       "4  c4-train-0000000004  [6685476, 2415781, 1112347, 742646, 6898911, 4..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import LSH\n",
    "from nemo_curator.utils.fuzzy_dedup_utils.id_mapping import convert_str_id_to_int\n",
    "\n",
    "lsh_input_dir = os.path.join(base_dir,\"00_c4_minhash\")\n",
    "id_field = 'id'\n",
    "output_bucket_dir = expand_outdir_and_mkdir(os.path.join(base_dir,\"01_c4_fuzzy_dedup_output\"))\n",
    "num_bands = 20\n",
    "buckets_per_shuffle = 1\n",
    "minhash_field = '_minhash_signature'\n",
    "minhash_length = 260\n",
    "log_dir = os.path.join(base_dir, \"NEMO_DATA/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH took 857.9130506515503 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Load MinHash output\n",
    "df = dask_cudf.read_parquet(lsh_input_dir, blocksize=\"2GB\", aggregate_files=True)\n",
    "df = df.map_partitions(\n",
    "    convert_str_id_to_int,\n",
    "    id_column=id_field,\n",
    "    meta=cudf.DataFrame(\n",
    "        {minhash_field: [[1, 2, 3]], \"doc_id\": [1], \"dataset_id\": np.uint32(1)}\n",
    "    ),\n",
    ")\n",
    "\n",
    "lsh = LSH(\n",
    "    cache_dir=output_bucket_dir,\n",
    "    num_hashes=minhash_length,\n",
    "    num_buckets=num_bands,\n",
    "    buckets_per_shuffle=buckets_per_shuffle,\n",
    "    id_fields=[\"dataset_id\", \"doc_id\"],\n",
    "    minhash_field=minhash_field,\n",
    "    logger=log_dir,\n",
    ")\n",
    "\n",
    "lsh_result = lsh(DocumentDataset(df))\n",
    "print(f\"LSH took {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>_bucket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>36525884</td>\n",
       "      <td>6050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>52458138</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>37051556</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>60789048</td>\n",
       "      <td>5475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>43778021</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id    doc_id  _bucket_id\n",
       "0  2191958705  36525884        6050\n",
       "1  2191958705  52458138          77\n",
       "2  2191958705  37051556           2\n",
       "3  2191958705  60789048        5475\n",
       "4  2191958705  43778021        1359"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_result.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import _MapBuckets\n",
    "from nemo_curator.utils.fuzzy_dedup_utils.io_utils import (\n",
    "    get_bucket_ddf_from_parquet_path,\n",
    "    get_text_ddf_from_json_path_with_blocksize,\n",
    ")\n",
    "\n",
    "input_data_paths = [os.path.join(base_dir,\"clean_c4_en_cleaned\")]\n",
    "num_files = None\n",
    "text_ddf_blocksize = 256 #The block size for chunking jsonl files for text ddf in mb\n",
    "id_field = 'id'\n",
    "text_field = 'text'\n",
    "input_bucket_path = os.path.join(base_dir,\"01_c4_fuzzy_dedup_output/_buckets.parquet\")\n",
    "input_bucket_field = '_bucket_id'\n",
    "shuffle_type ='tasks'\n",
    "log_dir = os.path.join(base_dir, \"logs\")\n",
    "output_anchor_docs_with_bk_path = expand_outdir_and_mkdir(os.path.join(base_dir,\"01_c4_fuzzy_dedup_output/anchor_docs_with_bk.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files being read for jaccard calculation = 1792\n",
      "ddf_text.npartitions  = 896\n"
     ]
    }
   ],
   "source": [
    "ddf_text = get_text_ddf_from_json_path_with_blocksize(\n",
    "    input_data_paths=input_data_paths,\n",
    "    num_files=num_files,\n",
    "    blocksize=text_ddf_blocksize,\n",
    "    id_column=id_field,\n",
    "    text_column=text_field,\n",
    ")\n",
    "\n",
    "print(f\"ddf_text.npartitions  = {ddf_text.npartitions}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ddf_bk partitions = 4\n",
      "Mapping Bucket took 88.8824942111969 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "num_workers = get_num_workers(gpu_client)\n",
    "\n",
    "# Read \"_buckets.parquet\"\n",
    "ddf_bk = get_bucket_ddf_from_parquet_path(\n",
    "    input_bucket_path=input_bucket_path, \n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "#Run _MapBuckets()\n",
    "map_buckets = _MapBuckets(\n",
    "    id_fields=[\"dataset_id\", \"doc_id\"], \n",
    "    bucket_field=input_bucket_field, \n",
    "    logger=log_dir,\n",
    "    text_field=text_field,\n",
    ")\n",
    "\n",
    "ddf_anchor_docs_with_bk = map_buckets.map_buckets_with_anchors(\n",
    "    documents_df=ddf_text, \n",
    "    buckets_df=ddf_bk, \n",
    "    shuffle_type=shuffle_type\n",
    ")\n",
    "\n",
    "#Write to disk\n",
    "ddf_anchor_docs_with_bk.to_parquet(\n",
    "    output_anchor_docs_with_bk_path, \n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "print(f\"Mapping Bucket took {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>anchor_1_dataset_id</th>\n",
       "      <th>anchor_1_doc_id</th>\n",
       "      <th>anchor_0_dataset_id</th>\n",
       "      <th>anchor_0_doc_id</th>\n",
       "      <th>_output_partition_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>32549647</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>31358312</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>89594282</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>35098816</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>85047220</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>51389075</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>40595502</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>15042562</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>61602848</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>20786631</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>62139963</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>640427</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2191958705</td>\n",
       "      <td>51827304</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>51827304</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>76096690</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id    doc_id  anchor_1_dataset_id  anchor_1_doc_id  \\\n",
       "0  2191958705  32549647           2191958705         31358312   \n",
       "1  2191958705  35098816           2191958705         85047220   \n",
       "2  2191958705  40595502           2191958705         15042562   \n",
       "3  2191958705  20786631           2191958705         62139963   \n",
       "4  2191958705  51827304           2191958705         51827304   \n",
       "\n",
       "   anchor_0_dataset_id  anchor_0_doc_id  _output_partition_id  \n",
       "0           2191958705         89594282                     9  \n",
       "1           2191958705         51389075                     5  \n",
       "2           2191958705         61602848                    10  \n",
       "3           2191958705           640427                     9  \n",
       "4           2191958705         76096690                    12  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_anchor_docs_with_bk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import _Shuffle\n",
    "\n",
    "log_dir = os.path.join(base_dir, \"logs\")\n",
    "input_anchor_docs_with_bk_path = os.path.join(base_dir,\"01_c4_fuzzy_dedup_output/anchor_docs_with_bk.parquet\")\n",
    "output_shuffled_docs_path = expand_outdir_and_mkdir(\n",
    "    os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/shuffled_docs.parquet\")\n",
    ")\n",
    "bucket_mapping_ddf_blocksize = 256\n",
    "parts_per_worker = 16\n",
    "bucket_parts_per_worker = 256\n",
    "id_field = 'id'\n",
    "text_field = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started processing bucket-map partitions 0 through 4 of 4\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2994466 rows to disk\n",
      "Text-df partition  64/896 completed in 31.12666344642639\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2999951 rows to disk\n",
      "Text-df partition  128/896 completed in 33.10323452949524\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2988432 rows to disk\n",
      "Text-df partition  192/896 completed in 33.50847387313843\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2990629 rows to disk\n",
      "Text-df partition  256/896 completed in 33.49195647239685\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2986183 rows to disk\n",
      "Text-df partition  320/896 completed in 31.32063102722168\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2992957 rows to disk\n",
      "Text-df partition  384/896 completed in 31.166403770446777\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2988685 rows to disk\n",
      "Text-df partition  448/896 completed in 33.60498857498169\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2980398 rows to disk\n",
      "Text-df partition  512/896 completed in 35.292184352874756\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2997635 rows to disk\n",
      "Text-df partition  576/896 completed in 33.02913284301758\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2989748 rows to disk\n",
      "Text-df partition  640/896 completed in 32.31175899505615\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2994604 rows to disk\n",
      "Text-df partition  704/896 completed in 36.09462070465088\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2986642 rows to disk\n",
      "Text-df partition  768/896 completed in 34.29168701171875\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2982089 rows to disk\n",
      "Text-df partition  832/896 completed in 33.2143931388855\n",
      "Using 64 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 2995991 rows to disk\n",
      "Text-df partition  896/896 completed in 33.716525077819824\n",
      "Bucket partition  4/4 completed in 465.3312621116638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:45<00:00, 465.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Shuffle took 465.44443917274475 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "shuffle = _Shuffle(\n",
    "    id_fields=[\"dataset_id\", \"doc_id\"],\n",
    "    text_field=text_field,\n",
    "    int_to_str_id=id_field,\n",
    "    logger=log_dir,\n",
    ")\n",
    "\n",
    "shuffle.shuffle_docs_on_buckets(\n",
    "    documents_df=ddf_text,\n",
    "    bucket_w_anchors_path=input_anchor_docs_with_bk_path,\n",
    "    output_shuffled_docs_path=output_shuffled_docs_path,\n",
    "    bucket_mapping_df_blocksize=bucket_mapping_ddf_blocksize,\n",
    "    parts_per_worker=parts_per_worker,\n",
    "    bucket_parts_per_worker=bucket_parts_per_worker,\n",
    "    partition_on=\"_output_partition_id\",\n",
    ")\n",
    "\n",
    "print(f\"Jaccard Shuffle took {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>_text_bytes</th>\n",
       "      <th>id</th>\n",
       "      <th>anchor_0_id</th>\n",
       "      <th>anchor_1_id</th>\n",
       "      <th>_output_partition_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a placeholder page for Sarah Benesh, w...</td>\n",
       "      <td>488</td>\n",
       "      <td>2191958705-5801337</td>\n",
       "      <td>2191958705-59295672</td>\n",
       "      <td>2191958705-35642137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Columbus Hotel Tenerife is free HD wallpap...</td>\n",
       "      <td>271</td>\n",
       "      <td>2191958705-6360534</td>\n",
       "      <td>2191958705-69858464</td>\n",
       "      <td>2191958705-62562517</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here at Low Water Pressure Guys, we will be re...</td>\n",
       "      <td>2361</td>\n",
       "      <td>2191958705-5498196</td>\n",
       "      <td>2191958705-41702786</td>\n",
       "      <td>2191958705-52826321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baby Gate Guys will be there for all your goal...</td>\n",
       "      <td>2336</td>\n",
       "      <td>2191958705-2697785</td>\n",
       "      <td>2191958705-73574607</td>\n",
       "      <td>2191958705-12714317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Los Angeles Rams Baby Clothes is free HD wallp...</td>\n",
       "      <td>273</td>\n",
       "      <td>2191958705-214268</td>\n",
       "      <td>2191958705-77047710</td>\n",
       "      <td>2191958705-69858464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  _text_bytes  \\\n",
       "0  This is a placeholder page for Sarah Benesh, w...          488   \n",
       "1  The Columbus Hotel Tenerife is free HD wallpap...          271   \n",
       "2  Here at Low Water Pressure Guys, we will be re...         2361   \n",
       "3  Baby Gate Guys will be there for all your goal...         2336   \n",
       "4  Los Angeles Rams Baby Clothes is free HD wallp...          273   \n",
       "\n",
       "                   id          anchor_0_id          anchor_1_id  \\\n",
       "0  2191958705-5801337  2191958705-59295672  2191958705-35642137   \n",
       "1  2191958705-6360534  2191958705-69858464  2191958705-62562517   \n",
       "2  2191958705-5498196  2191958705-41702786  2191958705-52826321   \n",
       "3  2191958705-2697785  2191958705-73574607  2191958705-12714317   \n",
       "4   2191958705-214268  2191958705-77047710  2191958705-69858464   \n",
       "\n",
       "  _output_partition_id  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_shuffle_res = dd.read_parquet(os.path.join(output_shuffled_docs_path,\"_output_partition_id=0\"))\n",
    "jaccard_shuffle_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import JaccardSimilarity\n",
    "\n",
    "id_field = 'id'\n",
    "text_field = 'text'\n",
    "ngram_size = 5\n",
    "shuffled_docs_path = os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/shuffled_docs.parquet\")\n",
    "jaccard_results_path = expand_outdir_and_mkdir(\n",
    "    os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/jaccard_similarity_results.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Computing+Writing took 16.437445402145386 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "jaccard = JaccardSimilarity(\n",
    "    id_field=id_field ,\n",
    "    text_field=text_field,\n",
    "    anchor_id_fields=[f\"anchor_{i}_{id_field}\" for i in range(2)],\n",
    "    ngram_width=ngram_size,\n",
    ")\n",
    "\n",
    "# Run actual computation\n",
    "result_df = jaccard.jaccard_compute(shuffled_docs_path)\n",
    "\n",
    "result_df.to_parquet(\n",
    "    jaccard_results_path,\n",
    "    write_index=False,\n",
    "    write_metadata_file=False,\n",
    ")\n",
    "\n",
    "print(f\"Jaccard Computing+Writing took {time.time() - t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>id_y</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-7885144</td>\n",
       "      <td>2191958705-9304226</td>\n",
       "      <td>0.885671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2191958705-11805797</td>\n",
       "      <td>2191958705-11093133</td>\n",
       "      <td>0.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2191958705-10927567</td>\n",
       "      <td>2191958705-8191823</td>\n",
       "      <td>0.685083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2191958705-12284408</td>\n",
       "      <td>2191958705-6891291</td>\n",
       "      <td>0.865922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2191958705-18809012</td>\n",
       "      <td>2191958705-12002623</td>\n",
       "      <td>0.956098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id_x                 id_y   jaccard\n",
       "0   2191958705-7885144   2191958705-9304226  0.885671\n",
       "1  2191958705-11805797  2191958705-11093133  0.898551\n",
       "2  2191958705-10927567   2191958705-8191823  0.685083\n",
       "3  2191958705-12284408   2191958705-6891291  0.865922\n",
       "4  2191958705-18809012  2191958705-12002623  0.956098"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_compute_res = dd.read_parquet(jaccard_results_path)\n",
    "jaccard_compute_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import ConnectedComponents\n",
    "\n",
    "cache_dir = expand_outdir_and_mkdir(\n",
    "    os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/cc-cache\")\n",
    ")\n",
    "jaccard_pairs_path = os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/jaccard_similarity_results.parquet\")\n",
    "id_field = 'id'\n",
    "jaccard_threshold = 0.8\n",
    "output_path = expand_outdir_and_mkdir(\n",
    "    os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/connected_components.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Component took 18.83707284927368 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "components_stage = ConnectedComponents(\n",
    "    cache_dir=cache_dir,\n",
    "    jaccard_pairs_path=jaccard_pairs_path,\n",
    "    id_column=id_field,\n",
    "    # convert_str_ids=True,\n",
    "    jaccard_threshold=jaccard_threshold,\n",
    ")\n",
    "components_stage.cc_workflow(output_path=output_path)\n",
    "print(f\"Connected Component took {time.time()-t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_id(df):\n",
    "    # Convert to cudf DataFrame if it's not already\n",
    "    df = cudf.DataFrame(df)\n",
    "    # Split the id into components\n",
    "    df['dataset_id'] = df['id'].astype('string').str.split('-').str.get(0)\n",
    "    df['doc_id'] = df['id'].astype('string').str.split('-').str.get(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-51036593</td>\n",
       "      <td>2191958705-51036593</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>51036593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-48768313</td>\n",
       "      <td>2191958705-48768313</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>48768313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-40155685</td>\n",
       "      <td>2191958705-40155685</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>40155685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-3159672</td>\n",
       "      <td>2191958705-3159672</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>3159672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2191958705-3276793</td>\n",
       "      <td>2191958705-3276793</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>3276793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id               id_str  dataset_id    doc_id\n",
       "group                                                                \n",
       "0      2191958705-51036593  2191958705-51036593  2191958705  51036593\n",
       "0      2191958705-48768313  2191958705-48768313  2191958705  48768313\n",
       "0      2191958705-40155685  2191958705-40155685  2191958705  40155685\n",
       "0       2191958705-3159672   2191958705-3159672  2191958705   3159672\n",
       "0       2191958705-3276793   2191958705-3276793  2191958705   3276793"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = os.path.join(base_dir, \"01_c4_fuzzy_dedup_output/connected_components.parquet\")\n",
    "cc_result = dask_cudf.read_parquet(output_path, split_row_groups=False).repartition(npartitions=1)\n",
    "\n",
    "df = cc_result.compute()\n",
    "df['id_str'] = df['id'].astype(str)\n",
    "df[['dataset_id', 'doc_id']] = df['id_str'].str.split('-', expand=True)\n",
    "cc_result = dask_cudf.from_cudf(df, npartitions=1)\n",
    "\n",
    "# Set 'group' as the index and shuffle to ensure all same 'group' values are in the same partition\n",
    "cc_result = cc_result.set_index('group', shuffle='tasks')\n",
    "\n",
    "cc_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of docs to remove = 4252681\n"
     ]
    }
   ],
   "source": [
    "# drop the id_str column\n",
    "cc_result = cc_result.drop(columns=['id_str'])\n",
    "\n",
    "# Define a function to assign cumulative counts and filter duplicates\n",
    "def assign_cumcount(df):\n",
    "    df['cumcount'] = df.groupby(level=0).cumcount()\n",
    "    df = df[df['cumcount'] >= 1]\n",
    "    df = df.drop(columns=['cumcount'])\n",
    "    return df\n",
    "\n",
    "# Find duplicates by applying the function to each partition\n",
    "docs_to_remove = cc_result.map_partitions(assign_cumcount, meta=cc_result)\n",
    "\n",
    "# Reset the index\n",
    "docs_to_remove = docs_to_remove.reset_index()\n",
    "\n",
    "docs_to_remove = docs_to_remove[[\"dataset_id\", \"doc_id\"]]\n",
    "docs_to_remove = docs_to_remove.rename(columns={\"dataset_id\":\"to_remove_dataset_id\", \"doc_id\":\"to_remove_doc_id\"})\n",
    "docs_to_remove = docs_to_remove.reset_index(drop=True).persist()\n",
    "_ = wait(docs_to_remove)\n",
    "del _ \n",
    "\n",
    "print(\"num of docs to remove =\", len(docs_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>119096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140650</th>\n",
       "      <td>24922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594161</th>\n",
       "      <td>13994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "group          \n",
       "11       119096\n",
       "4         58505\n",
       "0         26948\n",
       "4140650   24922\n",
       "1594161   13994"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_grouped = cc_result.groupby('group').agg({'doc_id': 'count'}).rename(columns={'doc_id': 'count'}).sort_values('count', ascending=False).compute()\n",
    "cc_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2191958705-83685852</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>83685852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2191958705-76068122</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>76068122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2191958705-81946381</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>81946381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2191958705-83353108</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>83353108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2191958705-43359258</td>\n",
       "      <td>2191958705</td>\n",
       "      <td>43359258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  dataset_id    doc_id\n",
       "group                                           \n",
       "11     2191958705-83685852  2191958705  83685852\n",
       "11     2191958705-76068122  2191958705  76068122\n",
       "11     2191958705-81946381  2191958705  81946381\n",
       "11     2191958705-83353108  2191958705  83353108\n",
       "11     2191958705-43359258  2191958705  43359258"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_group = cc_result.loc[11].compute()\n",
    "dup_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1792 files\n"
     ]
    }
   ],
   "source": [
    "# read input dataset\n",
    "input_data_dir = os.path.join(base_dir, \"clean_c4_en_cleaned\")\n",
    "input_dataset = DocumentDataset.read_json(input_data_dir, add_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for near duplicate examples with specific IDs took 1158.019103050232 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dup_ids = [\n",
    "    'c4-train-0083685852',\n",
    "    'c4-train-0076068122',\n",
    "    'c4-train-0081946381',\n",
    "    'c4-train-0083353108',\n",
    "    'c4-train-0043359258'\n",
    "] \n",
    "dup_examples = input_dataset.df[input_dataset.df['id'].isin(dup_ids)].compute()\n",
    "print(f\"Searching for near duplicate examples with specific IDs took {time.time()-t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>c4-train00851.jsonl</td>\n",
       "      <td>c4-train-0043359258</td>\n",
       "      <td>EN</td>\n",
       "      <td>This is a placeholder page for Mary Dean, whic...</td>\n",
       "      <td>2019-04-18 10:39:38</td>\n",
       "      <td>http://stmaryshighschool.net/phoenix-az/alumni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19473</th>\n",
       "      <td>c4-train01494.jsonl</td>\n",
       "      <td>c4-train-0076068122</td>\n",
       "      <td>EN</td>\n",
       "      <td>This is a placeholder page for Brittany Bokenk...</td>\n",
       "      <td>2019-04-24 22:17:20</td>\n",
       "      <td>http://comeauxhighschool.org/alumni/1134075/br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44507</th>\n",
       "      <td>c4-train01609.jsonl</td>\n",
       "      <td>c4-train-0081946381</td>\n",
       "      <td>EN</td>\n",
       "      <td>This is a placeholder page for Tim Howell, whi...</td>\n",
       "      <td>2019-04-22 10:20:58</td>\n",
       "      <td>http://grosseilehighschool.com/alumni/1876277/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25729</th>\n",
       "      <td>c4-train01637.jsonl</td>\n",
       "      <td>c4-train-0083353108</td>\n",
       "      <td>EN</td>\n",
       "      <td>This is a placeholder page for Lily Clements, ...</td>\n",
       "      <td>2019-04-20 18:16:05</td>\n",
       "      <td>http://polytechnichighschool.net/san_francisco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>c4-train01644.jsonl</td>\n",
       "      <td>c4-train-0083685852</td>\n",
       "      <td>EN</td>\n",
       "      <td>This is a placeholder page for Joshua Szafran,...</td>\n",
       "      <td>2019-04-23 04:18:59</td>\n",
       "      <td>http://griswoldhighschool.org/jewett_city-ct/a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename                   id language  \\\n",
       "41271  c4-train00851.jsonl  c4-train-0043359258       EN   \n",
       "19473  c4-train01494.jsonl  c4-train-0076068122       EN   \n",
       "44507  c4-train01609.jsonl  c4-train-0081946381       EN   \n",
       "25729  c4-train01637.jsonl  c4-train-0083353108       EN   \n",
       "2112   c4-train01644.jsonl  c4-train-0083685852       EN   \n",
       "\n",
       "                                                    text           timestamp  \\\n",
       "41271  This is a placeholder page for Mary Dean, whic... 2019-04-18 10:39:38   \n",
       "19473  This is a placeholder page for Brittany Bokenk... 2019-04-24 22:17:20   \n",
       "44507  This is a placeholder page for Tim Howell, whi... 2019-04-22 10:20:58   \n",
       "25729  This is a placeholder page for Lily Clements, ... 2019-04-20 18:16:05   \n",
       "2112   This is a placeholder page for Joshua Szafran,... 2019-04-23 04:18:59   \n",
       "\n",
       "                                                     url  \n",
       "41271  http://stmaryshighschool.net/phoenix-az/alumni...  \n",
       "19473  http://comeauxhighschool.org/alumni/1134075/br...  \n",
       "44507  http://grosseilehighschool.com/alumni/1876277/...  \n",
       "25729  http://polytechnichighschool.net/san_francisco...  \n",
       "2112   http://griswoldhighschool.org/jewett_city-ct/a...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example duplicate 1\n",
      "This is a placeholder page for Mary Dean, which means this person is not currently on this site. We do suggest using the tools below to find Mary Dean.\n",
      "You are visiting the placeholder page for Mary Dean. This page is here because someone used our placeholder utility to look for Mary Dean. We created this page automatically in hopes Mary Dean would find it. If you are not Mary Dean, but are an alumni of St Marys High School Phoenix, AZ, register on this site for free now.\n",
      "\n",
      "\n",
      "Example duplicate 2\n",
      "This is a placeholder page for Brittany Bokenkamp, which means this person is not currently on this site. We do suggest using the tools below to find Brittany Bokenkamp.\n",
      "You are visiting the placeholder page for Brittany Bokenkamp. This page is here because someone used our placeholder utility to look for Brittany Bokenkamp. We created this page automatically in hopes Brittany Bokenkamp would find it. If you are not Brittany Bokenkamp, but are an alumni of Comeaux High School, register on this site for free now.\n",
      "\n",
      "\n",
      "Example duplicate 3\n",
      "This is a placeholder page for Tim Howell, which means this person is not currently on this site. We do suggest using the tools below to find Tim Howell.\n",
      "You are visiting the placeholder page for Tim Howell. This page is here because someone used our placeholder utility to look for Tim Howell. We created this page automatically in hopes Tim Howell would find it. If you are not Tim Howell, but are an alumni of Grosse Ile High School, register on this site for free now.\n",
      "\n",
      "\n",
      "Example duplicate 4\n",
      "This is a placeholder page for Lily Clements, which means this person is not currently on this site. We do suggest using the tools below to find Lily Clements.\n",
      "You are visiting the placeholder page for Lily Clements. This page is here because someone used our placeholder utility to look for Lily Clements. We created this page automatically in hopes Lily Clements would find it. If you are not Lily Clements, but are an alumni of Polytechnic High School San Francisco, CA, register on this site for free now.\n",
      "\n",
      "\n",
      "Example duplicate 4\n",
      "This is a placeholder page for Joshua Szafran, which means this person is not currently on this site. We do suggest using the tools below to find Joshua Szafran.\n",
      "You are visiting the placeholder page for Joshua Szafran. This page is here because someone used our placeholder utility to look for Joshua Szafran. We created this page automatically in hopes Joshua Szafran would find it. If you are not Joshua Szafran, but are an alumni of Griswold High School Jewett City, CT, register on this site for free now.\n"
     ]
    }
   ],
   "source": [
    "print('Example duplicate 1\\n' + dup_examples.text.iloc[0])\n",
    "print('\\n\\nExample duplicate 2\\n' + dup_examples.text.iloc[1])\n",
    "print('\\n\\nExample duplicate 3\\n' + dup_examples.text.iloc[2])\n",
    "print('\\n\\nExample duplicate 4\\n' + dup_examples.text.iloc[3])\n",
    "print('\\n\\nExample duplicate 4\\n' + dup_examples.text.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1792 files\n"
     ]
    }
   ],
   "source": [
    "from helper import convert_str_id_to_int\n",
    "\n",
    "input_dataset = DocumentDataset.read_json(os.path.join(base_dir, \"clean_c4_en_cleaned\"), backend=\"cudf\")\n",
    "input_df = input_dataset.df[['text','id']]\n",
    "meta = input_df._meta\n",
    "meta['doc_id']=np.int64([0])\n",
    "meta['dataset_id']=np.uint32([0])\n",
    "input_df = input_df.map_partitions(\n",
    "    convert_str_id_to_int,\n",
    "    id_column=\"id\",\n",
    "    meta=meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicates and writing deduped dataset took 145.94984221458435 seconds\n"
     ]
    }
   ],
   "source": [
    "dedup_output_dir = expand_outdir_and_mkdir(os.path.join(base_dir, \"02_c4_deduped\"))\n",
    "\n",
    "input_df['doc_id'] = input_df['doc_id'].astype(str)\n",
    "input_df['dataset_id'] = input_df['dataset_id'].astype(str)\n",
    "docs_to_remove['to_remove_doc_id'] = docs_to_remove['to_remove_doc_id'].astype(str)\n",
    "docs_to_remove['to_remove_dataset_id'] = docs_to_remove['to_remove_dataset_id'].astype(str)\n",
    "\n",
    "deduped_df = input_df.merge(docs_to_remove,\n",
    "                             left_on=['doc_id','dataset_id'],\n",
    "                             right_on=[\"to_remove_doc_id\", \"to_remove_dataset_id\"],\n",
    "                             how='left')\n",
    "\n",
    "deduped_df = deduped_df[deduped_df['to_remove_doc_id'].isna()].drop(columns=['to_remove_doc_id', \"to_remove_dataset_id\"]).reset_index(drop=True)\n",
    "\n",
    "t0 = time.time()\n",
    "deduped_df.to_parquet(dedup_output_dir)\n",
    "print(f\"Removing duplicates and writing deduped dataset took {time.time()-t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86606272"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deduped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90858953"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90858953 - 86606272 = 4,258,681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 15:22:31,928 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:46383' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('stackpartition-3813775c5b01487c742e46afdb7bcbef', 3)} (stimulus_id='handle-worker-cleanup-1730906551.928179')\n",
      "2024-11-06 15:22:31,969 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41665' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('operation-fused-reset_index-e32eec7a6344f5f6f60d5403e6b82aa4', 0), ('stackpartition-3813775c5b01487c742e46afdb7bcbef', 0)} (stimulus_id='handle-worker-cleanup-1730906551.969132')\n",
      "2024-11-06 15:22:31,972 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:35815' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('stackpartition-3813775c5b01487c742e46afdb7bcbef', 1)} (stimulus_id='handle-worker-cleanup-1730906551.9722552')\n",
      "2024-11-06 15:22:31,974 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45739' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('stackpartition-3813775c5b01487c742e46afdb7bcbef', 2)} (stimulus_id='handle-worker-cleanup-1730906551.9740832')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 15:22:36,093 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-11-06 15:22:36,096 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# end the gpu client\n",
    "gpu_client.cluster.close()\n",
    "gpu_client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Workers = 128\n"
     ]
    }
   ],
   "source": [
    "scheduler_address = os.getenv(\"SCHEDULER_ADDRESS\")\n",
    "cpu_client = get_client(scheduler_address=scheduler_address)\n",
    "print(f\"Num Workers = {get_num_workers(cpu_client)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_curator\n",
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "\n",
    "filter_config_file = os.path.join(base_dir, \"NEMO_DATA/config/heuristic_filter_en.yaml\")\n",
    "hf_input_data_dir = os.path.join(base_dir, \"02_c4_deduped\")\n",
    "kept_document_dir =  expand_outdir_and_mkdir(os.path.join(base_dir,'03_c4_heuristic_filtering','hf.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1792 files\n",
      "Writing to disk complete for 1792 partitions\n",
      "Time taken for Heuristic filtering: 844.9139394760132 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Load dataset\n",
    "dataset = DocumentDataset.read_parquet(hf_input_data_dir)\n",
    "\n",
    "# construct pipeline from config\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "\n",
    "# filter data and write to disk\n",
    "filtered_dataset = filter_pipeline(dataset)\n",
    "filtered_dataset.to_parquet(kept_document_dir)\n",
    "\n",
    "print(f\"Time taken for Heuristic filtering: {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77910950"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_dataframe_complement\n",
    "\n",
    "original_df = dd.read_parquet(hf_input_data_dir)\n",
    "filtered_df = dd.read_parquet(kept_document_dir)\n",
    "removed_df = get_dataframe_complement(original_df, filtered_df)\n",
    "removed_df_example = removed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rich Reviews initial goal is to provide a shop window for authors with an Oxford link. The Oxford Times is a great promoter of local talent and we will seek to complement this respected outlet by giving priority to those self-­published authors with the talent to be recognised by the established publishing houses.\n",
      "We will attempt to be honest and positive in our reviews recognising that the achievement in producing a work of fact or fiction is generally a labour of love and fully deserving of a balanced appraisal. We will also be looking to associate the site with many of the local book clubs in the Oxford area giving those who have an opinion on all things literary the opportunity to have a voice and tell us what is good and bad in their reading groups.\n",
      "Ultimately this is a new venture and we will be looking to review as many books as possible. We look forward to hearing from you and hope to keep you entertained not only through our reviews but also via our blog.\n",
      "I am a book lover who enjoys writing hence the creation of this Rich Reviews website. I don't aspire to be the next editor of the Times Literary Supplement but want to create a literary forum for Oxfordshire people and beyond.\n",
      "I've lived in the Oxfordshire area for over 10 years, having previously been a student at Oxford Brookes University. I've spent time working abroad and am now settled in the Oxfordshire countryside with my partner and children.\n",
      "By day, I'm an online gaming professional, which, sadly, doesn't involve a life of casinos and living by the throw of the dice, however does involve IT and management of people. In an ideal world may be the former would be more exotic but I'm hoping this site will provide the creativity and fun often missing from the more mundane activities associated with a computer and office work.\n",
      "Legend 100 is a reading club – membership allows me to review new titles on my website in advance of publication. Click on the logo to find out more about Legend Press Publishing.\n"
     ]
    }
   ],
   "source": [
    "print(removed_df_example.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris, unexpectedly, decided we should head north on the weekend. *All* the way north. To Cape Reinga. Any further north, and we would have had to swim.\n",
      "Of some significance (to me, the highways nerd) was that this is the start marker for SH1, which runs the length of both islands to south of Bluff. It's more than 2000km long. I've travelled the length of the North Island, but I'm yet to make it south of the Cook Strait.\n",
      "As Chris has a new car to 'run in', we went via the scenic east coast on the way up, and the scenic west coast on the way back. The east coast roads tend to run closer to the coast. Along SH12 on the west coast, there are more wiggly roads and plenty of trees. I think Chris liked driving there better!\n",
      "Doubtless Bay is freakin' gorgeous. It's on SH10 as you travel up the northeast coast.\n",
      "Anyway, Cape Reinga is at least 6 hours north of Auckland (depends how fast you drive, and how many campervans/tourists you get stuck behind). It's of great significance to the Maori population, as it is believed to be where their spirits leave New Zealand to return to Hawaiki.\n",
      "The site has had an upgrade since I first went there nine years ago. Firstly, SH1 is not gravel for the last 25km anymore (*that* was an adventure, especially combined with tour buses and single lane bridges). Secondly, they have built some lovely architectural features (even the toilets are pretty).\n",
      "... and a sign that tells you how far you are from the rest of the world.\n",
      "We stayed at a little town called Ahipara, which is at the south end of 90 Mile Beach. We were not brave enough to take a non-4WD along the sands - so rather boringly, we drove on SH1.\n",
      "It was a beautiful place. Unfortunately Kaitaia, which is the nearest larger town, is a dump. I wouldn't recommend it to tourists. But staying on the beach nearby was worth the slightly larger investment.\n",
      "And I love these things.\n",
      "Opononi was my favourite beach on the drive back.\n",
      "It was a lot of driving for one weekend, but it was worth it to get out of town and get some fresh air!\n",
      "Nice post Kat - great pics too.\n"
     ]
    }
   ],
   "source": [
    "print(removed_df_example.text.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/neeleshg23/c4_curated/commit/main (Request ID: Root=1-672c2d20-4780009417faba6b461f20b2;fc616f2e-11bc-4cef-a909-308122ac590f)\n\nYou have been rate-limited; you can retry this action in about 1 hour. If you're a new user, your limits will raise progressively over time. Get in touch with us at website@huggingface.co if you need access now.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/neeleshg23/c4_curated/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 64\u001b[0m\n\u001b[1;32m     56\u001b[0m     api\u001b[38;5;241m.\u001b[39mupload_file(\n\u001b[1;32m     57\u001b[0m         path_or_fileobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m         path_in_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m     60\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mpush_large_parquet_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkept_document_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneeleshg23/c4_curated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 27\u001b[0m, in \u001b[0;36mpush_large_parquet_dataset\u001b[0;34m(filtered_dataset_dir, repo_id)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, parquet_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parquet_files):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Upload the parquet file directly\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     remote_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploaded file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(parquet_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Add dataset metadata\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1559\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4751\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4743\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4744\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4745\u001b[0m )\n\u001b[1;32m   4746\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   4747\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   4748\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   4749\u001b[0m )\n\u001b[0;32m-> 4751\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4764\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1559\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4093\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   4092\u001b[0m     commit_resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mcommit_url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 4093\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4095\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/neeleshg23/c4_curated/commit/main (Request ID: Root=1-672c2d20-4780009417faba6b461f20b2;fc616f2e-11bc-4cef-a909-308122ac590f)\n\nYou have been rate-limited; you can retry this action in about 1 hour. If you're a new user, your limits will raise progressively over time. Get in touch with us at website@huggingface.co if you need access now."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset, Dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def push_large_parquet_dataset(filtered_dataset_dir, repo_id):\n",
    "    \"\"\"\n",
    "    Push large parquet dataset to HF handling files in chunks\n",
    "    \n",
    "    Args:\n",
    "        filtered_dataset_dir: Directory containing parquet files\n",
    "        repo_id: \"username/dataset-name\"\n",
    "    \"\"\"\n",
    "    # Create the repository first\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "    \n",
    "    # Get list of all parquet files\n",
    "    parquet_files = glob.glob(os.path.join(filtered_dataset_dir, \"*.parquet\"))\n",
    "    \n",
    "    # Process each file separately\n",
    "    for i, parquet_file in enumerate(parquet_files):\n",
    "        # Upload the parquet file directly\n",
    "        remote_path = f\"data/part_{i:05d}.parquet\"\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=parquet_file,\n",
    "            path_in_repo=remote_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        print(f\"Uploaded file {i+1}/{len(parquet_files)}: {parquet_file}\")\n",
    "        \n",
    "    # Add dataset metadata\n",
    "    metadata = {\n",
    "        \"total_files\": len(parquet_files),\n",
    "        \"data_files\": [f\"data/part_{i:05d}.parquet\" for i in range(len(parquet_files))]\n",
    "    }\n",
    "    \n",
    "    # Create a dataset card\n",
    "    readme_content = f\"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# C4 Curated Dataset\n",
    "This dataset contains curated and filtered documents from the C4 dataset.\n",
    "\n",
    "Total number of parquet files: {len(parquet_files)}\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"README.md\",\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "push_large_parquet_dataset(kept_document_dir, \"neeleshg23/c4_curated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
